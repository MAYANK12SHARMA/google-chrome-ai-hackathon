{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_article_content(url):\n",
    "    try:\n",
    "        # Send an HTTP GET request\n",
    "        response = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "        response.raise_for_status()  # Raise error for HTTP issues\n",
    "        \n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Find all <article> tags and extract their text\n",
    "        articles = soup.find_all('article')\n",
    "        article_content = \"\\n\".join([article.get_text(strip=True) for article in articles])\n",
    "        \n",
    "        # Find all <p> tags and extract their text\n",
    "        paragraphs = soup.find_all('p')\n",
    "        paragraph_content = \"\\n\".join([p.get_text(strip=True) for p in paragraphs])\n",
    "        \n",
    "        # Combine and return the content\n",
    "        return article_content + \"\\n\\n\" + paragraph_content\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {e}\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def generate_markdown(raw_text, output_file=\"output.md\"):\n",
    "    # Step 1: Clean the raw text\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', raw_text)  # Normalize whitespace\n",
    "    cleaned_text = re.sub(r'(?<!\\.)\\n', ' ', cleaned_text)  # Replace line breaks with spaces\n",
    "    \n",
    "    # Step 2: Extract and format sections\n",
    "    # A simple heuristic to identify headers and content\n",
    "    cleaned_text = cleaned_text.replace(\"Good to know:\", \"\\n### Good to know:\\n\")\n",
    "    cleaned_text = re.sub(r'(?<!\\w)([A-Z][a-zA-Z0-9 ]+):', r'\\n## \\1\\n', cleaned_text)  # Convert headers\n",
    "    cleaned_text = re.sub(r'([.?!])\\s*([A-Z])', r'\\1\\n\\n\\2', cleaned_text)  # Split into sentences\n",
    "    \n",
    "    # Step 3: Handle bullets and lists\n",
    "    cleaned_text = re.sub(r'\\b([A-Za-z0-9]+[.])\\s', r'- \\1 ', cleaned_text)  # Turn into bullet points\n",
    "    \n",
    "    # Step 4: Extract links (if any) and add to the end of the file\n",
    "    links = re.findall(r'https?://\\S+', raw_text)\n",
    "    if links:\n",
    "        links_section = \"\\n\\n## Links\\n\" + \"\\n\".join(f\"- {link}\" for link in links)\n",
    "    else:\n",
    "        links_section = \"\"\n",
    "    \n",
    "    # Step 5: Write the Markdown content to a file\n",
    "    markdown_content = cleaned_text + links_section\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as md_file:\n",
    "        md_file.write(markdown_content)\n",
    "\n",
    "    return markdown_content\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "url = \"https://nextjs.org/docs/app/getting-started/project-structure\"\n",
    "content = extract_article_content(url)\n",
    "with open(\"content.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Markdown file generated successfully! Check 'mayank_output.md'\n"
     ]
    }
   ],
   "source": [
    "with open(\"content.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    raw_text = file.read()\n",
    "\n",
    "# Generate Markdown content\n",
    "markdown_content = generate_markdown(raw_text, output_file=\"mayank_output.md\")\n",
    "\n",
    "print(\"Markdown file generated successfully! Check 'mayank_output.md'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "403 Client Error: Forbidden for url: https://medium.com/nybles/inside-my-google-step-internship-fd78d1cdcf55",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 73\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;66;03m# Use a file\u001b[39;00m\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# process_html(\"mayank.txt\", is_url=False, output_file=\"mayank_output.md\", tags_to_extract=[\"article\", \"p\", \"h1\"])\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;66;03m# process_html(\"https://nextjs.org/docs\", is_url=True, output_file=\"nextjs_docs.md\", tags_to_extract=[\"article\", \"p\", \"h1\"])\u001b[39;00m\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;66;03m# process_html(\"https://nextjs.org/docs/app/building-your-application/configuring/environment-variables\", is_url=True, output_file=\"nextjs_docs1.md\", tags_to_extract=[\"article\", \"p\", \"h1\"])\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m     \u001b[43mprocess_html\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhttps://medium.com/nybles/inside-my-google-step-internship-fd78d1cdcf55\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_url\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmedium.md\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags_to_extract\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43marticle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mp\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mh1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 52\u001b[0m, in \u001b[0;36mprocess_html\u001b[1;34m(input_source, is_url, output_file, tags_to_extract)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# Step 1: Fetch HTML content\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_url:\n\u001b[1;32m---> 52\u001b[0m     raw_html \u001b[38;5;241m=\u001b[39m \u001b[43mfetch_html_from_url\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_source\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(input_source, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n",
      "Cell \u001b[1;32mIn[1], line 14\u001b[0m, in \u001b[0;36mfetch_html_from_url\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;124;03mFetch the HTML content of a webpage from a given URL.\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;124;03m:param url: The URL of the webpage to fetch.\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;124;03m:return: The raw HTML content as a string.\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     13\u001b[0m response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(url)\n\u001b[1;32m---> 14\u001b[0m \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Raise an error for HTTP issues\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mtext\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\requests\\models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1016\u001b[0m     http_error_msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1017\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Server Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreason\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for url: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1018\u001b[0m     )\n\u001b[0;32m   1020\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1021\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 403 Client Error: Forbidden for url: https://medium.com/nybles/inside-my-google-step-internship-fd78d1cdcf55"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from markdownify import markdownify as md\n",
    "\n",
    "# Function to fetch HTML from a URL\n",
    "def fetch_html_from_url(url):\n",
    "    \"\"\"\n",
    "    Fetch the HTML content of a webpage from a given URL.\n",
    "    \n",
    "    :param url: The URL of the webpage to fetch.\n",
    "    :return: The raw HTML content as a string.\n",
    "    \"\"\"\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  # Raise an error for HTTP issues\n",
    "    return response.text\n",
    "\n",
    "# Function to extract specific HTML tags and clean up\n",
    "def extract_clean_html(html_content, tags_to_extract=None, minimal_css=True):\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    if tags_to_extract is None:\n",
    "        tags_to_extract = [tag.name for tag in soup.find_all()]\n",
    "    extracted_content = []\n",
    "    for tag in tags_to_extract:\n",
    "        for element in soup.find_all(tag):\n",
    "            if minimal_css:\n",
    "                for attr in ['style', 'class', 'id']:\n",
    "                    if attr in element.attrs:\n",
    "                        del element.attrs[attr]\n",
    "            extracted_content.append(element)\n",
    "    return \"\\n\".join(str(tag) for tag in extracted_content)\n",
    "\n",
    "# Function to convert HTML to Markdown and save to a file\n",
    "def convert_html_to_markdown(html_content, output_file=\"output.md\"):\n",
    "    markdown_content = md(html_content, strip=['a'])\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(markdown_content)\n",
    "    return markdown_content\n",
    "\n",
    "# Complete workflow for both file and URL\n",
    "def process_html(input_source, is_url=False, output_file=\"output.md\", tags_to_extract=None):\n",
    "    \"\"\"\n",
    "    Process HTML content from a file or URL, extract relevant tags, \n",
    "    convert to Markdown, and save it.\n",
    "    \n",
    "    :param input_source: File path or URL of the HTML content.\n",
    "    :param is_url: Set to True if input_source is a URL.\n",
    "    :param output_file: Name of the output Markdown file.\n",
    "    :param tags_to_extract: List of tags to extract (optional).\n",
    "    \"\"\"\n",
    "    # Step 1: Fetch HTML content\n",
    "    if is_url:\n",
    "        raw_html = fetch_html_from_url(input_source)\n",
    "    else:\n",
    "        with open(input_source, \"r\", encoding=\"utf-8\") as file:\n",
    "            raw_html = file.read()\n",
    "\n",
    "    # Step 2: Extract and clean HTML\n",
    "    cleaned_html = extract_clean_html(raw_html, tags_to_extract, minimal_css=True)\n",
    "\n",
    "    # Step 3: Convert to Markdown and save\n",
    "    markdown_output = convert_html_to_markdown(cleaned_html, output_file=output_file)\n",
    "    print(f\"Markdown file generated successfully! Check '{output_file}'\")\n",
    "    return markdown_output\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Use a file\n",
    "    # process_html(\"mayank.txt\", is_url=False, output_file=\"mayank_output.md\", tags_to_extract=[\"article\", \"p\", \"h1\"])\n",
    "\n",
    "    # Use a URL\n",
    "    # process_html(\"https://nextjs.org/docs\", is_url=True, output_file=\"nextjs_docs.md\", tags_to_extract=[\"article\", \"p\", \"h1\"])\n",
    "    # process_html(\"https://nextjs.org/docs/app/building-your-application/configuring/environment-variables\", is_url=True, output_file=\"nextjs_docs1.md\", tags_to_extract=[\"article\", \"p\", \"h1\"])\n",
    "    process_html(\"https://medium.com/nybles/inside-my-google-step-internship-fd78d1cdcf55\", is_url=True, output_file=\"medium.md\", tags_to_extract=[\"article\", \"p\", \"h1\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NoSuchDriverException",
     "evalue": "Message: Unable to obtain driver for chrome; For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors/driver_location\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\selenium\\webdriver\\common\\driver_finder.py:64\u001b[0m, in \u001b[0;36mDriverFinder._binary_paths\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Path(path)\u001b[38;5;241m.\u001b[39mis_file():\n\u001b[1;32m---> 64\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe path is not a valid file: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_paths[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdriver_path\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m path\n",
      "\u001b[1;31mValueError\u001b[0m: The path is not a valid file: /path/to/chromedriver",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mNoSuchDriverException\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 72\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     71\u001b[0m     url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://medium.com/nybles/inside-my-google-step-internship-fd78d1cdcf55\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 72\u001b[0m     \u001b[43mprocess_html_from_url\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmedium_article.md\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags_to_extract\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43marticle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mp\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mh1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[5], line 58\u001b[0m, in \u001b[0;36mprocess_html_from_url\u001b[1;34m(url, output_file, tags_to_extract)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_html_from_url\u001b[39m(url, output_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput.md\u001b[39m\u001b[38;5;124m\"\u001b[39m, tags_to_extract\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;66;03m# Step 1: Fetch HTML content using Selenium\u001b[39;00m\n\u001b[1;32m---> 58\u001b[0m     raw_html \u001b[38;5;241m=\u001b[39m \u001b[43mfetch_html_with_selenium\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;66;03m# Step 2: Extract and clean HTML content\u001b[39;00m\n\u001b[0;32m     61\u001b[0m     cleaned_html \u001b[38;5;241m=\u001b[39m extract_clean_html(raw_html, tags_to_extract, minimal_css\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[5], line 18\u001b[0m, in \u001b[0;36mfetch_html_with_selenium\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Specify the location of the ChromeDriver\u001b[39;00m\n\u001b[0;32m     17\u001b[0m service \u001b[38;5;241m=\u001b[39m Service(executable_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/path/to/chromedriver\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# Replace with the path to your ChromeDriver\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m driver \u001b[38;5;241m=\u001b[39m \u001b[43mwebdriver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mChrome\u001b[49m\u001b[43m(\u001b[49m\u001b[43mservice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mservice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchrome_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Navigate to the URL\u001b[39;00m\n\u001b[0;32m     21\u001b[0m driver\u001b[38;5;241m.\u001b[39mget(url)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\selenium\\webdriver\\chrome\\webdriver.py:45\u001b[0m, in \u001b[0;36mWebDriver.__init__\u001b[1;34m(self, options, service, keep_alive)\u001b[0m\n\u001b[0;32m     42\u001b[0m service \u001b[38;5;241m=\u001b[39m service \u001b[38;5;28;01mif\u001b[39;00m service \u001b[38;5;28;01melse\u001b[39;00m Service()\n\u001b[0;32m     43\u001b[0m options \u001b[38;5;241m=\u001b[39m options \u001b[38;5;28;01mif\u001b[39;00m options \u001b[38;5;28;01melse\u001b[39;00m Options()\n\u001b[1;32m---> 45\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbrowser_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDesiredCapabilities\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCHROME\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbrowserName\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvendor_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgoog\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mservice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mservice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_alive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\selenium\\webdriver\\chromium\\webdriver.py:50\u001b[0m, in \u001b[0;36mChromiumDriver.__init__\u001b[1;34m(self, browser_name, vendor_prefix, options, service, keep_alive)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mservice \u001b[38;5;241m=\u001b[39m service\n\u001b[0;32m     49\u001b[0m finder \u001b[38;5;241m=\u001b[39m DriverFinder(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mservice, options)\n\u001b[1;32m---> 50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mfinder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_browser_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m     51\u001b[0m     options\u001b[38;5;241m.\u001b[39mbinary_location \u001b[38;5;241m=\u001b[39m finder\u001b[38;5;241m.\u001b[39mget_browser_path()\n\u001b[0;32m     52\u001b[0m     options\u001b[38;5;241m.\u001b[39mbrowser_version \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\selenium\\webdriver\\common\\driver_finder.py:47\u001b[0m, in \u001b[0;36mDriverFinder.get_browser_path\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_browser_path\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_binary_paths\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbrowser_path\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\selenium\\webdriver\\common\\driver_finder.py:78\u001b[0m, in \u001b[0;36mDriverFinder._binary_paths\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m     77\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to obtain driver for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbrowser\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 78\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NoSuchDriverException(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_paths\n",
      "\u001b[1;31mNoSuchDriverException\u001b[0m: Message: Unable to obtain driver for chrome; For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors/driver_location\n"
     ]
    }
   ],
   "source": [
    "%pip install selenium\n",
    "\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "from markdownify import markdownify as md\n",
    "\n",
    "# Function to fetch HTML content using Selenium (for dynamic pages like Medium)\n",
    "def fetch_html_with_selenium(url):\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")  # Run in headless mode (no browser UI)\n",
    "    chrome_options.add_argument(\"--disable-gpu\")  # Disable GPU for better performance\n",
    "    chrome_options.add_argument(\"--no-sandbox\")  # Bypass OS security model\n",
    "    \n",
    "    # Specify the location of the ChromeDriver\n",
    "    service = Service(executable_path='/path/to/chromedriver')  # Replace with the path to your ChromeDriver\n",
    "    driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "    \n",
    "    # Navigate to the URL\n",
    "    driver.get(url)\n",
    "    \n",
    "    # Wait for the page to load completely (you may adjust the sleep time based on the site)\n",
    "    time.sleep(5)\n",
    "    \n",
    "    # Get the page source after JavaScript has been rendered\n",
    "    html_content = driver.page_source\n",
    "    \n",
    "    driver.quit()  # Close the browser\n",
    "    \n",
    "    return html_content\n",
    "\n",
    "# Function to extract specific HTML tags and clean up\n",
    "def extract_clean_html(html_content, tags_to_extract=None, minimal_css=True):\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    if tags_to_extract is None:\n",
    "        tags_to_extract = [tag.name for tag in soup.find_all()]\n",
    "    extracted_content = []\n",
    "    for tag in tags_to_extract:\n",
    "        for element in soup.find_all(tag):\n",
    "            if minimal_css:\n",
    "                for attr in ['style', 'class', 'id']:\n",
    "                    if attr in element.attrs:\n",
    "                        del element.attrs[attr]\n",
    "            extracted_content.append(element)\n",
    "    return \"\\n\".join(str(tag) for tag in extracted_content)\n",
    "\n",
    "# Function to convert HTML to Markdown and save to a file\n",
    "def convert_html_to_markdown(html_content, output_file=\"output.md\"):\n",
    "    markdown_content = md(html_content, strip=['a'])\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(markdown_content)\n",
    "    return markdown_content\n",
    "\n",
    "# Complete workflow for processing HTML from URL (using Selenium)\n",
    "def process_html_from_url(url, output_file=\"output.md\", tags_to_extract=None):\n",
    "    # Step 1: Fetch HTML content using Selenium\n",
    "    raw_html = fetch_html_with_selenium(url)\n",
    "    \n",
    "    # Step 2: Extract and clean HTML content\n",
    "    cleaned_html = extract_clean_html(raw_html, tags_to_extract, minimal_css=True)\n",
    "    \n",
    "    # Step 3: Convert to Markdown and save\n",
    "    markdown_output = convert_html_to_markdown(cleaned_html, output_file=output_file)\n",
    "    \n",
    "    print(f\"Markdown file generated successfully! Check '{output_file}'\")\n",
    "    return markdown_output\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    url = \"https://medium.com/nybles/inside-my-google-step-internship-fd78d1cdcf55\"\n",
    "    process_html_from_url(url, output_file=\"medium_article.md\", tags_to_extract=[\"article\", \"p\", \"h1\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched HTML content successfully\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Function to fetch HTML content with custom headers to bypass 403 errors\n",
    "def fetch_html_from_url(url):\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "    }\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()  # Raise an exception for HTTP errors\n",
    "        return response.text\n",
    "    except requests.exceptions.HTTPError as err:\n",
    "        print(f\"HTTP error occurred: {err}\")\n",
    "    except Exception as err:\n",
    "        print(f\"Other error occurred: {err}\")\n",
    "\n",
    "# Now you can use this function to fetch the HTML content of Medium or any other website.\n",
    "url = \"https://medium.com/nybles/inside-my-google-step-internship-fd78d1cdcf55\"\n",
    "html_content = fetch_html_from_url(url)\n",
    "if html_content:\n",
    "    print(\"Fetched HTML content successfully\")\n",
    "else:\n",
    "    print(\"Failed to fetch content\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content successfully saved to medium_content.md\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Function to fetch HTML content with custom headers to bypass 403 errors\n",
    "def fetch_html_from_url(url):\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "    }\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()  # Raise an exception for HTTP errors (like 403)\n",
    "        return response.text  # Return the raw HTML content\n",
    "    except requests.exceptions.HTTPError as err:\n",
    "        print(f\"HTTP error occurred: {err}\")  # Print error if HTTP request fails\n",
    "        return None\n",
    "    except Exception as err:\n",
    "        print(f\"Other error occurred: {err}\")  # Print error if something else fails\n",
    "        return None\n",
    "\n",
    "# Function to process the HTML content and extract specified tags\n",
    "def extract_tags_from_html(html, tags_to_extract):\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    # Initialize a string to store the extracted content\n",
    "    extracted_content = \"\"\n",
    "    \n",
    "    # Loop through each specified tag and extract its content\n",
    "    for tag in tags_to_extract:\n",
    "        for element in soup.find_all(tag):\n",
    "            extracted_content += f\"{element.get_text(strip=True)}\\n\\n\"\n",
    "    \n",
    "    return extracted_content\n",
    "\n",
    "# Function to save the extracted content as a markdown file\n",
    "def save_to_markdown(content, output_file):\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(content)\n",
    "    print(f\"Content successfully saved to {output_file}\")\n",
    "\n",
    "# Main function to process URL and extract content\n",
    "def process_html(input_source, is_url=True, output_file=\"output.md\", tags_to_extract=[\"article\", \"p\", \"h1\"]):\n",
    "    if is_url:\n",
    "        # Fetch HTML from the URL\n",
    "        raw_html = fetch_html_from_url(input_source)\n",
    "        if raw_html:\n",
    "            extracted_content = extract_tags_from_html(raw_html, tags_to_extract)\n",
    "            save_to_markdown(extracted_content, output_file)\n",
    "        else:\n",
    "            print(f\"Failed to retrieve HTML from {input_source}\")\n",
    "    else:\n",
    "        # Read from file if not a URL\n",
    "        with open(input_source, \"r\", encoding=\"utf-8\") as file:\n",
    "            raw_html = file.read()\n",
    "            extracted_content = extract_tags_from_html(raw_html, tags_to_extract)\n",
    "            save_to_markdown(extracted_content, output_file)\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    url = \"https://medium.com/nybles/inside-my-google-step-internship-fd78d1cdcf55\"  # Replace with your URL\n",
    "    process_html(url, is_url=True, output_file=\"medium_content.md\", tags_to_extract=[\"article\", \"p\", \"h1\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
